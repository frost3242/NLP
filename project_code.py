# -*- coding: utf-8 -*-
"""PROJECT_CODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GHOHlAlcggOwnbO75nCnDSKImdZ0lSwg
"""

import os
import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
from time import sleep
from requests.exceptions import RequestException

input_file = "Input.xlsx"  # Replace with the actual path if needed
input_data = pd.read_excel(input_file)

# Create a folder to store the output text files
output_folder = "Extracted_Articles"
os.makedirs(output_folder, exist_ok=True)

# Function to extract the article title and text from a given URL
def extract_article(url, retries=3, delay=5):
    for attempt in range(retries):
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()  # Check for HTTP errors
            soup = BeautifulSoup(response.content, 'html.parser')

            # Extract the article title and text (adjust selectors as per website structure)
            title = soup.find('h1')  # Assuming title is in <h1> tag
            paragraphs = soup.find_all('p')  # Assuming text is in <p> tags

            if title:
                title_text = title.get_text(strip=True)
            else:
                title_text = "No Title Found"

            article_text = "\n".join(p.get_text(strip=True) for p in paragraphs)
            return title_text, article_text
        except RequestException as e:
            print(f"Error fetching {url}: {e}. Retrying in {delay} seconds...")
            sleep(delay)
    return None, None

# Process each row in the Excel file
for index, row in input_data.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    print(f"Processing URL_ID {url_id}: {url}")
    title, text = extract_article(url)

    if title and text:
        # Save the extracted data to a text file named after URL_ID
        file_path = os.path.join(output_folder, f"{url_id}.txt")
        with open(file_path, 'w', encoding='utf-8') as file:
            file.write(f"{title}\n\n{text}")
        print(f"Saved: {file_path}")
    else:
        print(f"Failed to extract: {url}")

print("Extraction process complete.")

pip install -q textstat

import nltk
nltk.download('punkt_tab')

from textstat import textstat
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize

# Ensure necessary NLTK data is downloaded
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Define file paths
input_file_path = 'Input.xlsx'
output_file_path = 'Output Data Structure.xlsx'

# Load input and output structure files
input_data = pd.read_excel(input_file_path)
output_structure = pd.read_excel(output_file_path)

# Create a folder to store extracted articles
output_folder = 'Extracted_Articles'
os.makedirs(output_folder, exist_ok=True)

# Helper functions
def clean_text(text):
    """Clean text by removing links, special characters, and extra spaces."""
    text = re.sub(r'http\S+|www\S+', '', text)  # Remove hyperlinks
    text = re.sub(r'[^\w\s.,!?]', '', text)    # Remove special characters
    text = re.sub(r'\s+', ' ', text).strip()    # Remove extra spaces
    return text

def count_syllables(word):
    """Count the number of syllables in a word."""
    return textstat.syllable_count(word)

def calculate_analysis_metrics(text):
    """Calculate required metrics for textual analysis."""
    sentences = sent_tokenize(text)
    words = word_tokenize(text)
    words = [word for word in words if word.isalnum()]
    stop_words = set(stopwords.words('english'))
    complex_words = [word for word in words if count_syllables(word) > 2]

    # Scores
    positive_score = sum(1 for word in words if word.lower() in positive_words)
    negative_score = sum(1 for word in words if word.lower() in negative_words)
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)

    # Metrics
    avg_sentence_length = sum(len(word_tokenize(sent)) for sent in sentences) / len(sentences)
    percentage_complex_words = len(complex_words) / len(words)
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)
    avg_words_per_sentence = len(words) / len(sentences)
    complex_word_count = len(complex_words)
    word_count = len(words)
    syllables_per_word = sum(count_syllables(word) for word in words) / len(words)
    personal_pronouns = len([word for word in words if word.lower() in ['i', 'we', 'my', 'ours', 'us']])
    avg_word_length = sum(len(word) for word in words) / len(words)

    return {
        "Positive Score": positive_score,
        "Negative Score": negative_score,
        "Polarity Score": polarity_score,
        "Subjectivity Score": subjectivity_score,
        "Average Sentence Length": avg_sentence_length,
        "Percentage of Complex Words": percentage_complex_words,
        "Fog Index": fog_index,
        "Average Number of Words per Sentence": avg_words_per_sentence,
        "Complex Word Count": complex_word_count,
        "Word Count": word_count,
        "Syllables per Word": syllables_per_word,
        "Personal Pronouns": personal_pronouns,
        "Average Word Length": avg_word_length
    }

# Load positive and negative words (replace with actual lists or files)
positive_words = ["good", "great", "excellent", "positive", "fortunate", "correct", "superior"]
negative_words = ["bad", "poor", "negative", "unfortunate", "wrong", "inferior"]

# Process each URL and compute metrics
results = []
for _, row in input_data.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    print(f"Processing URL_ID {url_id}: {url}")
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract title and content
        title = soup.find('h1').get_text(strip=True) if soup.find('h1') else "No Title"
        paragraphs = soup.find_all('p')
        article_text = " ".join(p.get_text(strip=True) for p in paragraphs)

        # Clean and analyze text
        article_text = clean_text(article_text)
        metrics = calculate_analysis_metrics(article_text)

        # Append results
        results.append({"URL_ID": url_id, **metrics})

    except Exception as e:
        print(f"Failed to process URL_ID {url_id}: {e}")

# Save results to a DataFrame
results_df = pd.DataFrame(results)

# Save to Excel in the same structure as the output file
results_df.to_excel('Output.xlsx', index=False)
print("Textual analysis complete. Results saved to 'Output.xlsx'.")

